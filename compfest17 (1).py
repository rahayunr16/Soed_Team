# -*- coding: utf-8 -*-
"""COMPFEST17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1euH1pdHoX9qPjlsjCFXxqkQv-nIoMBLV

<a href="https://colab.research.google.com/github/hyrahmaaa/Belajar-Penerapan-Data-Science-2/blob/main/notebook.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Data Science Academy COMPFEST 17 (Soed Team)

## Overview Proyek

Proyek ini berfokus pada pengembangan model prediksi konsumsi listrik harian (dalam satuan GWh) menggunakan data historis cuaca dan energi dari berbagai wilayah di Indonesia. Pemahaman dan prediksi pola konsumsi listrik sangat penting untuk efisiensi distribusi energi, pengendalian beban puncak, dan mitigasi risiko pemadaman.

**Latar Belakang:**

Listrik adalah kebutuhan fundamental yang konsumsinya sangat dipengaruhi oleh faktor cuaca seperti suhu, durasi sinar matahari, kecepatan angin, dan kelembapan udara. Fluktuasi ini memengaruhi kebutuhan energi suatu wilayah, sehingga kemampuan untuk memprediksi konsumsi listrik berdasarkan data cuaca menjadi krusial untuk perencanaan energi yang efisien dan berkelanjutan.

**Tujuan Analisis:**

Tujuan utama proyek ini adalah membangun model machine learning yang mampu memprediksi konsumsi listrik harian secara akurat.

**Data:**

Dataset yang akan digunakan berasal dari kaggle berikut : https://www.kaggle.com/competitions/seleksi-dsa-compfest-17/

**Manfaat:**

Proyek ini diharapkan dapat memberikan kontribusi nyata dalam:
- Efisiensi Energi Nasional: Optimalisasi penggunaan dan distribusi energi.
- Pengendalian Beban Puncak: Mengurangi risiko kelebihan beban dan pemadaman.
- Mitigasi Risiko Pemadaman: Perencanaan yang lebih baik untuk mencegah gangguan pasokan listrik.
- Pengelolaan Energi Masa Depan Indonesia: Mendukung keberlanjutan pasokan energi nasional.

## Persiapan

**MENYIAPKAN LIBRARY YANG DIBUTUHKAN**

**Fungsi cell ini:**
- Mengimport semua library yang diperlukan untuk analisis data
"""

! pip install streamlit

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import statsmodels.formula.api as ols
import statsmodels.api as sm
from google.colab import files
import streamlit
import joblib
import io
import pickle
import os

from scipy.stats import chi2_contingency
from itertools import combinations
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import GridSearchCV, train_test_split
import xgboost as xgb
from scipy.stats import skew
from google.colab import drive
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score


sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)

print("Semua library berhasil diimpor!")

"""**MENYIAPKAN DATA YANG DIGUNAKAN**

**Fungsi cell ini:**
- Membaca file CSV dataset tentang konsumsi listrik harian (submission.csv, train.csv, test.csv).
- Menampilkan informasi dasar tentang dataset
- Melihat dimensi data (jumlah baris dan kolom)
- Menampilkan daftar semua kolom dalam dataset
"""

# Unggah kaggle.json ke Google Colab
files.upload()

# Pindahkan kaggle.json ke lokasi yang benar dan ubah izin
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Unduh Data
!kaggle competitions download -c seleksi-dsa-compfest-17

# Ekstrak File
!unzip seleksi-dsa-compfest-17.zip

# Muat Data dan Eksplorasi Data Awal (EDA)
# Muat dataset train
df_train = pd.read_csv('train.csv')
print("Dataset 'train.csv' berhasil dimuat.")
print("\n--- DATASET TRAIN OVERVIEW ---")
print("=" * 50)
print(f"Shape: {df_train.shape[0]} rows, {df_train.shape[1]} columns")
print(f"Memory usage: {df_train.memory_usage(deep=True).sum() / 1024:.2f} KB")
print("\nColumns:")
for i, col in enumerate(df_train.columns, 1):
    print(f"  {i:2d}. {col}")

print("\n----------------------------------")

# Muat dataset test
df_test = pd.read_csv('test.csv')
print("\nDataset 'test.csv' berhasil dimuat.")
print("\n--- DATASET TEST OVERVIEW ---")
print("=" * 50)
print(f"Shape: {df_test.shape[0]} rows, {df_test.shape[1]} columns")
print(f"Memory usage: {df_test.memory_usage(deep=True).sum() / 1024:.2f} KB")
print("\nColumns:")
for i, col in enumerate(df_test.columns, 1):
    print(f"  {i:2d}. {col}")

print("\n----------------------------------")

df_train.head()

df_test.head()

"""## Data Understanding (Berdasarkan Train.csv)

**Fungsi cell ini:**

- Memahami fitur dataset pada bisnis masalah prediksi konsumsi listrik harian.
- Menganalisis struktur dan tipe data dalam dataset.
- Menganalisis statistika deskriptif pada kolom dataset.
- Mengecek missing values (data yang hilang).
- Mengecek nilai unik.
- Mengecek data duplikat.

**MEMAHAMI LABEL DATA:**

- `ID` : string. Identifikasi unik (format: <cluster_id>_<YYYY-MM-DD>)
- `date` : string. Format: YYYY-MM-DD.
- `cluster_id` : string. Identifikasi cluster (misal cluster_1 hingga cluster_4).
- `electricity_consumption` : float. (Hanya di train.csv) Konsumsi listrik harian dalam GWh.
- `temperature_2m_max` : float. Suhu maksimum di ketinggian 2 meter (°C).
- `temperature_2m_min` : float. Suhu minimum di ketinggian 2 meter (°C).
- `apparent_temperature_max` : float. Suhu terasakan maksimum (°C).
- `apparent_temperature_min` : float. Suhu terasakan minimum (°C).
- `sunshine_duration` : float. Durasi sinar matahari per hari (jam).
- `daylight_duration` : float. Durasi siang hari (detik).
- `wind_speed_10m_max` : float. Kecepatan angin maksimum di 10 meter (m/s).
- `wind_gusts_10m_max` : float. Kecepatan embusan angin maksimum di 10 meter (m/s)
- `wind_direction_10m_dominant` : int. 	Arah angin dominan di 10 meter (derajat dari utara)
- `shortwave_radiation_sum` : float. Total radiasi gelombang pendek (MJ/m²)
- `et0_fao_evapotranspiration` : float. Evapotranspirasi referensi FAO (Penman-Monteith) (mm).

**AKRONIM & SATUAN**

- GWh: Gigawatt-jam — satuan energi (1 GWh = 10⁹ watt-jam)
- GW: Gigawatt — satuan daya (1 GW = 10⁹ watt)
°C: Derajat Celsius
- m/s: Meter per detik
- MJ/m²: Megajoule per meter persegi
- FAO: Food and Agriculture Organization (Organisasi Pangan dan Pertanian PBB)

**STRUKTUR DAN TIPE DATA**
"""

print("\n--- Informasi Detail Data Train ---")
print(df_train.dtypes)

"""Diperoleh bahwa data df_train memiliki 11.688 baris dan 15 kolom. Dengan rincian kolom ber tipe float64 terdiri dari : `electricity_consumption`,`temperature_2m_max `,`temperature_2m_min`,`apparent_temperature_max`,`apparent_temperature_min`,`sunshine_duration`,`daylight_duration`,`wind_speed_10m_max`,`wind_gusts_10m_max `,`wind_direction_10m_dominant`,`shortwave_radiation_sum`,`et0_fao_evapotranspiration`. Dengan rincian kolom ber tipe object : `ID`,`date`,`cluster_id`.

Terdapat kolom `ID` harus dicek duplikasinya.

Terdapat kolom `date` coba dicek time seriesnya apakah teratur atau tidak dan seharusnya bertipe datetime.

Tidak terdapat kolom `electricity_consumption` pada  test.csv tetapi ada pada train.csv sehingga akan dilakukan pemodelan disini untuk mengisi kolom pada test.csv.

Terdapat kolom `wind_direction_10m_dominant` bernilai float tetapi pada deskripsi menyatakan int, perlu diubah tipenya.

**STATISTIKA DESKRIPTIF**
"""

print("\nStatistik Deskriptif Data Train:")
print(df_train.describe())

"""Terdapat anomali `sunshine_duraction` karena menunjukkan nilai maksimum yang sangat tidak masuk akal (puluhan ribu, jauh melampaui 24 jam sehari). Ini mengindikasikan kemungkinan besar adanya kesalahan satuan (unit error) yaitu dalam detik. Namun, untuk kesesuaian deskripsi ini dapat diubah menjadi satuan jam.

**CEK MISSING VALUE**
"""

print("\nJumlah Nilai Hilang di Data Train:")
print(df_train.isnull().sum())

"""Tidak terdapat nilai kosong sehingga tidak ada penanganan di pemrosesan data.

**NILAI UNIK**
"""

print("\n--- Nilai Unik per Kolom di Data Train ---")
print("=" * 50)

for col in df_train.columns:
    print(f"\nKolom '{col}':")

    if df_train[col].nunique() < 50:
        print(f"  Jumlah nilai unik: {df_train[col].nunique()}")
        print(f"  Nilai-nilai unik: {df_train[col].unique()}")
    else:
        print(f"  Jumlah nilai unik: {df_train[col].nunique()}")

print("\nIdentifikasi nilai unik selesai.")

"""Terdapat kolom `ID` yang menyatakan nilai unik atau identitas dari setiap baris. Adapun jumlah barisnya sama dengan jumlah nilai unik. Sehingga kemungkinan besar tidak ada data duplikat.

Untuk kolom `date` jumlah nilai unik hanya sekitar 200an sehingga mungkin terdapat loncat hari atau tiap cluster hanya terhitung 200 hari (tetapi masih kurang lengkap untuk masing-masing 4 cluster).

Untuk kolom `cluster_id` terdapat 4 cluster sehingga bisa eksplorasi untuk masing-masing cluster.

**CEK DATA DUPLIKAT**
"""

df_train.duplicated().sum()

"""Tidak terdapat data yang duplikat, sehingga tidak ada strategi penanganannya.

## Data Preparation / Preprocessing (Berdasarkan Train.csv)

**Fungsi cell ini:**

- Mengubah tipe data
- Penanganan missing values (data yang hilang).
- Penanganan data duplikat.
- Ubah nilai data dari kolom `sunshine_duration` terkait validasi satuan.
- Melakukan feature engineering.
- Melakukan EDA dengan rincian:
    - Analisis Univariate : histogram dari variabel kategorikal dan visualisasi dari variabel numerik, melihat boxplotnya dan mengatasi outlier jika ada, serta melihat time series dari kolom `date`.
    - Analisis Bivariate : melakukan analisis korelasi untuk variabel numerik dan kategorikal terkait hubungan dari `electricity_consumption`.
- Melakukan encoding data kategorikal.
- Melakukan split data untuk melatih model.
- Melakukan standarisasi

**MENGUBAH TIPE DATA**
"""

df_train['wind_direction_10m_dominant'] = df_train['wind_direction_10m_dominant'].round().astype('Int64')
df_train['date'] = pd.to_datetime(df_train['date'])

"""**PENANGANAN MISSING VALUE**

Tidak ada missing value ketika data understanding sehingga tidak dilakukan penanganan.

**PENANGANAN DATA DUPLIKAT**

Tidak ada data duplikat sehingga dapat dikatakan juga `ID` sebagai indeks dapat didrop.
"""

df_train1 = df_train.drop('ID', axis=1)
df_train1.head()

"""**UBAH SATUAN NILAI KOLOM `sunshine_duration`**"""

# --- Konversi sunshine_duration dari Detik ke Jam ---
# 1 jam = 3600 detik

df_train1['sunshine_duration'] = df_train1['sunshine_duration'] / 3600

print("\n--- Setelah Konversi sunshine_duration ke jam ---")
print(df_train1['sunshine_duration'])
print("\nStatistik deskriptif sunshine_duration setelah konversi:")
print(df_train1['sunshine_duration'].describe())

"""**MELAKUKAN FEATURE ENGINEERING**

Akan diubah kolom `date` menjadi beberapa kolom seperti `month`, `year`,`day`.
"""

df_train1['year'] = df_train1['date'].dt.year
df_train1['month'] = df_train1['date'].dt.month_name()
df_train1['day_of_week'] = df_train1['date'].dt.day_name()

print("\n--- DataFrame Setelah Penambahan Kolom Baru ---")
print(df_train1.head())

print("\n--- Cek Unik Nilai pada Kolom Baru (Tipe Kategorikal) ---")
print(f"Unik 'year': {df_train1['year'].unique()}")
print(f"Unik 'month': {df_train1['month'].unique()}")
print(f"Unik 'day_of_week': {df_train1['day_of_week'].unique()}")

"""**EXLORATORY DATA ANALYSIS**

**ANALISIS UNIVARIATE **

 -> analisis statistik yang berfokus pada eksplorasi dan deskripsi satu variabel (fitur) secara individual untuk memahami karakteristiknya.
"""

# ANALISIS KOLOM NUMERIK (HISTOGRAM)

# Pilih hanya kolom numerik
numerical_cols = df_train1.select_dtypes(include=np.number).columns.tolist()
num_cols = len(numerical_cols)
ncols = min(num_cols, 4)
nrows = (num_cols + ncols - 1) // ncols

# Figure dan Axes
fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 4 * nrows))

axes = axes.flatten()

for i, col in enumerate(numerical_cols):
    ax = axes[i]
    sns.histplot(df_train1[col], kde=True, ax=ax)
    ax.set_title(f'{col} (Skew: {df_train1[col].skew():.2f})', fontsize=10)
    ax.tick_params(axis='both', labelsize=8)

if num_cols < len(axes):
    for j in range(num_cols, len(axes)):
        fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""Insight:

* `electricity_consumption`: Distribusi cenderung positif miring (kanan) dengan beberapa puncak yang menunjukkan pola konsumsi bervariasi.
* `temperature_2m_max`: Distribusi hampir simetris dengan sedikit kemiringan positif, terpusat di sekitar 20-25°C.
* `temperature_2m_min`: Distribusi sangat simetris, terpusat di sekitar 5-10°C.
* `apparent_temperature_max`: Mirip dengan temperature_2m_max, agak miring positif dan terpusat di sekitar 20-25°C.
* `apparent_temperature_min`: Mirip dengan temperature_2m_min, distribusi sangat simetris, terpusat di sekitar 5-10°C.
* `sunshine_duration`: Distribusi sedikit miring negatif (kiri), menunjukkan lebih banyak hari dengan durasi sinar matahari yang lebih panjang.
* `daylight_duration`: Distribusi sangat simetris dan terpusat kuat di sekitar 40.000 - 45.000 detik (sekitar 11-12.5 jam).
* `wind_speed_10m_max`: Distribusi sangat miring positif, dengan sebagian besar data berada pada kecepatan angin rendah.
* `wind_gusts_10m_max`: Distribusi sangat miring positif, mirip dengan wind_speed_10m_max, didominasi embusan angin rendah.
* `wind_direction_10m_dominant`: Distribusi miring negatif dengan beberapa puncak, menunjukkan arah angin dominan bervariasi.
* `shortwave_radiation_sum`: Distribusi cenderung positif miring, dengan sebagian besar radiasi harian pada tingkat yang lebih rendah.
* `et0_fao_evapotranspiration`: Distribusi miring positif, menunjukkan sebagian besar hari memiliki nilai evapotranspirasi yang lebih rendah.
* `year`:terlihat memiliki tren tahunan.
"""

# ANALISIS KOLOM NUMERIK (BOXPLOT)

# Dapatkan semua kolom numerik
numerical_cols = df_train1.select_dtypes(include=np.number).columns.tolist()

# Buat box plot untuk setiap kolom numerik
plt.figure(figsize=(20, 15))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(5, 6, i)
    sns.boxplot(y=df_train1[col])
    plt.title(col, fontsize=10)
    plt.tight_layout()

plt.show()

"""Insight:

* `electricity_consumption`: Banyak outlier di sisi atas (konsumsi sangat tinggi); perlu ditangani karena dapat mengganggu model.
* `temperature_2m_max`: Beberapa outlier di kedua sisi (suhu ekstrem); pertimbangkan penanganan jika nilainya tidak realistis untuk lokasi (Indonesia), jika realistis, mungkin cukup transformasi.
* `temperature_2m_min`: Beberapa outlier di kedua sisi (suhu ekstrem); pertimbangkan penanganan jika nilainya tidak realistis (misal: negatif di Indonesia), jika realistis, mungkin cukup transformasi.
* `apparent_temperature_max`: Beberapa outlier di kedua sisi; pertimbangkan penanganan serupa dengan temperature_2m_max.
* `apparent_temperature_min`: Beberapa outlier di kedua sisi; pertimbangkan penanganan serupa dengan temperature_2m_min.
* `sunshine_duration`: Outlier di sisi atas (durasi sinar matahari sangat panjang); tidak terlalu mendesak untuk ditangani secara agresif karena kemungkinan valid secara fisik (hari sangat cerah).
* `daylight_duration`: Outlier di kedua sisi; tidak terlalu mendesak untuk ditangani karena durasi siang hari secara astronomis memiliki variasi yang valid.
* `wind_speed_10m_max`: Banyak outlier di sisi atas (kecepatan angin sangat tinggi); perlu ditangani  karena dapat mengganggu model.
* `wind_gusts_10m_max`: Banyak outlier di sisi atas (embusan angin sangat tinggi); perlu ditangani  serupa dengan wind_speed_10m_max.
* `wind_direction_10m_dominant`: Tidak ada outlier signifikan; tidak perlu ditangani.
* `shortwave_radiation_sum`: Banyak outlier di sisi atas (radiasi tinggi); perlu ditangani karena dapat mempengaruhi distribusi fitur.
* `et0_fao_evapotranspiration`: Banyak outlier di sisi atas (evapotranspirasi tinggi); perlu ditangani karena dapat mempengaruhi distribusi fitur.
* `year`: tidak memiliki outlier.

"""

# PENANGANAN OUTLIER

print("--- Statistik Deskriptif SEBELUM Capping IQR ---")
cols_to_cap = [
    'electricity_consumption',
    'wind_speed_10m_max',
    'wind_gusts_10m_max',
    'shortwave_radiation_sum',
    'et0_fao_evapotranspiration'
]
print(df_train1[cols_to_cap].describe())

# --- 1. Penanganan Outlier menggunakan Capping IQR (hanya batas atas) ---

for col in cols_to_cap:
    Q1 = df_train1[col].quantile(0.25)
    Q3 = df_train1[col].quantile(0.75)
    IQR = Q3 - Q1
    upper_bound = Q3 + 1.5 * IQR

    df_train1[col] = df_train1[col].clip(upper=upper_bound, lower=df_train1[col].min())
    print(f"Menerapkan capping IQR ke '{col}'. Batas Atas: {upper_bound:.2f}")

# --- 2. Verifikasi Setelah Penanganan Outlier (Capping IQR) ---
print("\n--- Statistik Deskriptif SETELAH Capping IQR ---")
print(df_train1[cols_to_cap].describe())

# --- Visualisasi ---
plt.figure(figsize=(15, 10))
for i, col in enumerate(cols_to_cap):
    plt.subplot(2, 3, i + 1)
    sns.boxplot(y=df_train1[col])
    plt.title(f'Capped: {col}')

plt.tight_layout()
plt.show()

print("\nDataFrame dengan kolom yang sudah dicapping (beberapa baris awal):")
print(df_train1[cols_to_cap].head())

categorical_cols = df_train1.select_dtypes(include=['object']).columns.tolist()

def plot_categorical_bar_charts(df, categorical_cols, cols_per_row=4):
    n_cols = len(categorical_cols)
    n_rows = int(np.ceil(n_cols / cols_per_row))

    fig, axes = plt.subplots(nrows=n_rows, ncols=cols_per_row, figsize=(cols_per_row * 5, n_rows * 4))

    axes = axes.flatten() if n_rows > 1 else np.array([axes]).flatten()

    for i, col in enumerate(categorical_cols):
        ax = axes[i]

        counts = df[col].value_counts()

        sns.barplot(x=counts.index, y=counts.values, ax=ax)

        ax.set_title(f'Distribusi {col}')
        ax.set_xlabel(col)
        ax.set_ylabel('Frekuensi')

        for container in ax.containers:
            ax.bar_label(container, fmt='%.0f')

        if len(counts.index) > 5 or counts.index.dtype == 'O':
             ax.tick_params(axis='x', rotation=45) #

        if col == 'date':

            df[col] = pd.to_datetime(df[col])
            ax.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m-%d'))
            fig.autofmt_xdate()


    if n_cols < n_rows * cols_per_row:
        for j in range(n_cols, n_rows * cols_per_row):
            fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

plot_categorical_bar_charts(df_train1, categorical_cols)

"""Insight:

* Distribusi `month`: Sebagian besar bulan memiliki frekuensi data yang sangat mirip, sekitar 992. Namun, ada sedikit penurunan frekuensi pada bulan Februari (sekitar 904). Ini mungkin karena Februari memiliki jumlah hari yang lebih sedikit, atau mungkin ada faktor musiman lain yang memengaruhi pengumpulan data atau kejadian pada bulan tersebut.

* Distribusi `day_of_week`: Distribusi frekuensi per hari dalam seminggu juga terlihat sangat merata, berkisar antara 1668 hingga 1672 data. Setiap hari dalam seminggu memiliki jumlah observasi yang hampir sama, yang berarti analisis pola harian tidak akan terdistorsi oleh perbedaan jumlah data antar hari.

* Distribusi `cluster_id`: Data terdistribusi dengan sangat merata di antara keempat `cluster_id` (cluster_1, cluster_2, cluster_3, cluster_4), dengan masing-masing cluster memiliki jumlah entri yang  sama.
"""

# ANALISIS KOLOM KATEGORIKAL (DIAGRAM LINGKARAN)

def plot_categorical_pie_charts(df_train1, categorical_cols, cols_per_row=4):
    n_cols = len(categorical_cols)
    n_rows = int(np.ceil(n_cols / cols_per_row))

    fig, axes = plt.subplots(nrows=n_rows, ncols=cols_per_row, figsize=(cols_per_row * 6, n_rows * 4))

    for i, col in enumerate(categorical_cols):
        row_idx = i // cols_per_row
        col_idx = i % cols_per_row

        if n_rows > 1:
            ax = axes[row_idx, col_idx]
        else:
            ax = axes[col_idx]

        # Menghitung frekuensi setiap kategori
        counts = df_train1[col].value_counts()
        labels = counts.index
        sizes = counts.values

        ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
        ax.set_title(f'Distribusi {col}')

    if n_cols < n_rows * cols_per_row:
        for j in range(n_cols, n_rows * cols_per_row):
            if n_rows > 1:
                fig.delaxes(axes.flatten()[j])
            else:
                fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

plot_categorical_pie_charts(df_train1, categorical_cols)

"""Insight:

* Distribusi `cluster_id`: Empat klaster terdistribusi sangat merata, masing-masing menyumbang 25% dari total data.

* Distribusi `month`: Distribusi data per bulan cukup merata, meskipun Februari dan November sedikit lebih kecil dengan 7.7% dan 8.2% dibandingkan bulan lainnya yang sekitar 8.5%.

* Distribusi `day_of_week`: Data terdistribusi sangat seimbang di antara hari-hari dalam seminggu, dengan masing-masing hari menyumbang sekitar 14.3%.
"""

clusters = ['cluster_1', 'cluster_2', 'cluster_3', 'cluster_4']

plt.figure(figsize=(15, 10))

for i, cluster in enumerate(clusters):
    df_train_cluster = df_train1[df_train1['cluster_id'] == cluster].copy()
    df_train_cluster = df_train_cluster.sort_values(by='date')

    plt.subplot(len(clusters), 1, i + 1)
    sns.lineplot(data=df_train_cluster, x='date', y='electricity_consumption')
    plt.title(f'Time Series untuk {cluster}')
    plt.xlabel('Tanggal')
    plt.ylabel('Konsumsi Listrik (GWh)')
    plt.grid(True)

plt.tight_layout()
plt.show()

"""Insight:

* Pola Musiman Tahunan yang Kuat (Strong Annual Seasonality): Ini adalah insight paling menonjol. Setiap tahun (antara 2014 hingga 2022), terlihat pola puncak dan lembah yang berulang. Ini sangat khas untuk data konsumsi listrik. Puncak konsumsi kemungkinan terjadi pada bulan-bulan tertentu (misalnya, musim panas saat penggunaan AC tinggi, atau musim liburan/perayaan besar), dan lembah pada bulan-bulan lain (misalnya, musim hujan atau saat aktivitas industri/perkantoran menurun).

**ANALISIS BIVARIATE**

-> Analisis statistik yang berfokus pada eksplorasi dan deskripsi hubungan antara dua variabel (fitur) secara bersamaan untuk memahami karakteristik keterkaitan atau interaksi di antara keduanya. Adapun kolom utama sebagai target prediksi model adalah `electricity_consumption`.
"""

# ANALISIS KOLOM NUMERIK VS ELECTRICITY_CONSUMPTION

# Pilih hanya kolom numerik
numerical_cols = df_train1.select_dtypes(include=np.number)

# Hitung matriks korelasi
correlation_matrix = numerical_cols.corr()

# Buat heatmap korelasi
plt.figure(figsize=(16, 14))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Heatmap Korelasi antar Semua Kolom Numerik', fontsize=16)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.tight_layout()
plt.show()

"""Insight:

* Korelasi Positif Kuat dengan Suhu (Temperatur):

`~temperature_2m_max`: 0.70 (Korelasi positif kuat)
`temperature_2m_min`: 0.56 (Korelasi positif sedang hingga kuat)
`apparent_temperature_max`: 0.76 (Korelasi positif kuat)
`apparent_temperature_min`: 0.60 (Korelasi positif sedang hingga kuat)

Insight: Semakin tinggi suhu (baik maksimum maupun minimum, dan suhu semu/yang dirasakan), semakin tinggi konsumsi listrik. Ini sangat logis dan umum terjadi, kemungkinan besar karena peningkatan penggunaan pendingin ruangan (AC) saat cuaca panas.

* Korelasi Negatif Sedang dengan Durasi Matahari/Siang Hari:

`sunshine_duration`: -0.23 (Korelasi negatif lemah hingga sedang)
`daylight_duration`: -0.24 (Korelasi negatif lemah hingga sedang)

Insight: Ada sedikit tendensi bahwa semakin lama durasi matahari bersinar atau durasi siang hari, konsumsi listrik cenderung sedikit menurun. Ini mungkin karena orang lebih banyak menggunakan cahaya alami, atau aktivitas luar ruangan lebih dominan. Namun, korelasi ini tidak sekuat korelasi dengan suhu.

* Korelasi Negatif Sedang dengan Radiasi Gelombang Pendek dan Evapotranspirasi:

`shortwave_radiation_sum`: -0.26 (Korelasi negatif lemah hingga sedang)
`et0_fao_evapotranspiration`: -0.26 (Korelasi negatif lemah hingga sedang)

Insight: Mirip dengan durasi matahari, peningkatan radiasi gelombang pendek (sinar matahari yang sampai ke permukaan) dan evapotranspirasi (penguapan air dari permukaan dan tumbuhan, yang terkait dengan ketersediaan energi dari matahari dan kelembaban) cenderung berkorelasi negatif lemah hingga sedang dengan konsumsi listrik. Ini bisa jadi karena saat kondisi ini tinggi, cuaca mungkin lebih cerah dan menyebabkan pola perilaku yang mengurangi konsumsi listrik (misalnya, orang lebih banyak di luar ruangan, penggunaan penerangan berkurang, dll.).

* Korelasi Negatif Lemah dengan Angin:

`wind_speed_10m_max`: -0.04 (Korelasi negatif sangat lemah)
`wind_gusts_10m_max`: -0.05 (Korelasi negatif sangat lemah)
`wind_direction_10m_dominant`: 0.01 (Korelasi positif sangat lemah, hampir nol)
`year` : -0.02 (Korelasi negatif sangat lemah)

Insight: Kecepatan dan hembusan angin memiliki korelasi yang sangat lemah (mendekati nol) dengan konsumsi listrik. Arah angin tidak memiliki korelasi yang signifikan sama sekali. Artinya, faktor angin tidak banyak berpengaruh pada konsumsi listrik.

"""

categorical_vars = ['cluster_id','month', 'day_of_week']
numeric_var = 'electricity_consumption'

print(f"\n--- Analisis Hubungan: Variabel Kategorikal vs. {numeric_var} ---")

for cat_var in categorical_vars:
    print(f"\n======== Analisis untuk: {cat_var} ========")

    # Uji ANOVA (Analisis Varians)
    # C(cat_var) memberitahu statsmodels bahwa cat_var adalah variabel kategorikal
    model = sm.formula.ols(f'{numeric_var} ~ C({cat_var})', data=df_train1).fit()
    anova_table = sm.stats.anova_lm(model, typ=2)

    print(f"\nHasil ANOVA untuk {cat_var}:")
    print(anova_table)

    p_value_anova = anova_table['PR(>F)'][0]
    if p_value_anova < 0.05:
        print(f"\nKesimpulan ANOVA: Terdapat perbedaan rata-rata {numeric_var} yang signifikan secara statistik antara setidaknya satu pasang kategori dalam {cat_var} (p < 0.05).")
        print(f"Melakukan Uji Post-Hoc (Tukey's HSD) untuk {cat_var} guna melihat pasangan mana yang berbeda:")

        from statsmodels.stats.multicomp import pairwise_tukeyhsd
        tukey_result = pairwise_tukeyhsd(endog=df_train1[numeric_var],
                                         groups=df_train1[cat_var],
                                         alpha=0.05)
        print(tukey_result)

        print("\nInterpretasi Tukey HSD: Kolom 'reject=True' menunjukkan ada perbedaan signifikan antara pasangan kelompok tersebut.")
    else:
        print(f"\nKesimpulan ANOVA: Tidak ada perbedaan rata-rata {numeric_var} yang signifikan secara statistik antara kategori-kategori dalam {cat_var} (p >= 0.05).")

    print("\n" + "="*50)

"""Insight:

Semua variabel kategorikal (`cluster_id`, `year`, `month`, dan `day_of_week`) menunjukkan korelasi/hubungan yang signifikan secara statistik dengan `electricity_consumption`, karena nilai p-value ANOVA mereka semua di bawah 0.05. Rincian:

* `cluster_id` menunjukkan perbedaan yang paling jelas, di mana setiap klaster memiliki rata-rata konsumsi yang berbeda dari klaster lainnya.

* `month` dan `day_of_week` juga menunjukkan perbedaan yang sangat kuat, mengindikasikan pola musiman dan pola harian (akhir pekan vs. hari kerja) yang memengaruhi konsumsi listrik.

**ENCODING KOLOM KATEGORIKAL**

Akan dilakukan encoding pada kolom kategorikal yaitu kolom `cluster_id`, `year`, `month`, `day_of_week` menggunakan jenis One-Hot-Encoding.
"""

categorical_columns_to_encode = ['cluster_id','month', 'day_of_week']

df_train1_encoded = pd.get_dummies(df_train1,
                            columns=categorical_columns_to_encode)

newly_created_dummy_cols = [col for col in df_train1_encoded.columns
                            if any(col.startswith(cat_col + '_') for cat_col in categorical_columns_to_encode)]

for col in newly_created_dummy_cols:
    if df_train1_encoded[col].dtype == bool:
        df_train1_encoded[col] = df_train1_encoded[col].astype(int)


print("\n--- DataFrame df_train1_encoded setelah One-Hot Encoding ---")
print(df_train1_encoded)
print("\nTipe data setelah encoding:")
print(df_train1_encoded.dtypes)

df_train1_encoded.shape

"""**MELAKUKAN SPLIT DATA UNTUK MELATIH MODEL DAN STANDARISASI**

Tujuan utama dari model machine learning bukanlah hanya untuk menghafal data pelatihan, melainkan untuk membuat prediksi yang akurat pada data baru yang belum pernah dilihat sebelumnya.

Tujuan dilakukan standarisasi adalah menyamakan skala fitur dan meningkatkan kinerja model tertentu.
"""

target_var = ['electricity_consumption']
feature_var = [col for col in df_train1_encoded.columns if col != target_var and col != 'date']

def standardize_data(df_train1_encoded, target_var, feature_var):

    X = df_train1_encoded[feature_var]
    y = df_train1_encoded[target_var].values.ravel()

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    scaler = StandardScaler()

    X_train_scaled = scaler.fit_transform(X_train)

    X_test_scaled = scaler.transform(X_test)

    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)

    return X_train_scaled_df, X_test_scaled_df, y_train, y_test, scaler

X_train_scaled, X_test_scaled, y_train, y_test, scaler = standardize_data(df_train1_encoded.copy(), target_var, feature_var)

print("\nData Latih yang Distandarisasi:")
print(X_train_scaled.head())
print("\nData Uji yang Distandarisasi:")
print(X_test_scaled.head())
print("\nTarget Latih:")
print(y_train)
print("\nTarget Uji:")
print(y_test)


scaler_filename = 'scaler.pkl'

try:
    joblib.dump(scaler, scaler_filename)
    print(f"Scaler berhasil disimpan sebagai '{scaler_filename}'")

    # CEK UKURAN FILE SCALER
    file_size_scaler = os.path.getsize(scaler_filename)
    print(f"Ukuran file scaler: {file_size_scaler} bytes")
    if file_size_scaler == 0:
        print("PERINGATAN: File scaler 0 bytes! Ada masalah saat menyimpan!")
    elif file_size_scaler < 500:
        print("INFO: File scaler biasanya kecil, ukuran ini wajar.")
except Exception as e:
    print(f"ERROR FATAL SAAT MENYIMPAN SCALER: {e}")

# --- Simpan Scaler (StandardScaler) ---
scaler_filename = 'electricity_consumption_standard_scaler.pkl'

try:
    joblib.dump(scaler, scaler_filename)
    print(f"Scaler berhasil disimpan sebagai '{scaler_filename}'")

    # CEK UKURAN FILE SCALER
    file_size_scaler = os.path.getsize(scaler_filename)
    print(f"Ukuran file scaler: {file_size_scaler} bytes")
    if file_size_scaler == 0:
        print("PERINGATAN: File scaler 0 bytes! Ada masalah saat menyimpan!")
    elif file_size_scaler < 500: # Scaler juga biasanya kecil
        print("INFO: File scaler biasanya kecil, ukuran ini wajar.")
except Exception as e:
    print(f"ERROR FATAL SAAT MENYIMPAN SCALER: {e}")

"""## Modeling

**PEMILIHAN MODEL**
"""

models = {
    'Linear Regression': LinearRegression(),
    'Random Forest Regressor': RandomForestRegressor(random_state=42),
    'XGBoost Regressor': XGBRegressor(random_state=42)
}

results = {}

# Latih dan evaluasi setiap model
for name, model in models.items():
    # Latih model
    model.fit(X_train_scaled, y_train)

    # Lakukan prediksi pada data uji
    y_pred = model.predict(X_test_scaled)

    # Calculate regression metrics instead of accuracy
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    results[name] = {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}

# Buat DataFrame untuk menampilkan hasil dalam bentuk tabel
results_df = pd.DataFrame.from_dict(results, orient='index')

# Cetak tabel hasil
print("\nPerbandingan Performa Model Regresi:")
print(results_df)

"""## Evaluation"""

print("\n--- PEMILIHAN MODEL DAN EVALUASI ---")

models = {
    'Linear Regression': LinearRegression(),
    'Random Forest Regressor': RandomForestRegressor(random_state=42),
}

try:
    import xgboost as xgb
    models['XGBoost Regressor'] = xgb.XGBRegressor(random_state=42, objective='reg:squarederror')
except ImportError:
    print("XGBoost belum terinstal, melompati model XGBoost Regressor.")

results = {}

for name, model in models.items():
    print(f"\n===== Melatih dan Mengevaluasi: {name} =====")

    # Latih model
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    # Hitung metrik evaluasi regresi
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    results[name] = {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}

    print(f"MAE: {mae:.4f}")
    print(f"MSE: {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"R2: {r2:.4f}")

    # --- Visualisasi Hasil ---
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 2, 1)
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('Nilai Aktual')
    plt.ylabel('Nilai Prediksi')
    plt.title(f'Prediksi vs. Aktual ({name})')
    plt.grid(True)

    plt.subplot(1, 2, 2)
    residuals = y_test - y_pred
    plt.scatter(y_pred, residuals, alpha=0.5)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('Nilai Prediksi')
    plt.ylabel('Residual')
    plt.title(f'Residual Plot ({name})')
    plt.grid(True)

    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(8, 5))
    sns.histplot(residuals, kde=True)
    plt.title(f'Distribusi Residual ({name})')
    plt.xlabel('Residual (Aktual - Prediksi)')
    plt.ylabel('Frekuensi')
    plt.grid(True)
    plt.show()


# --- Tampilkan Tabel Perbandingan Performa Model Regresi ---
results_df = pd.DataFrame(results).T
print("\nPerbandingan Performa Model Regresi:")
print(results_df)

"""Insight :    
Dari hasil yang diperoleh, akan dipilih model terbaik yaitu model Linear Regression.
"""

best_model_for_prediction = models['Linear Regression']
# --- Simpan Model Regresi (Linear Regression) ---
model_filename = 'electricity_consumption_linear_regression_model.pkl'

try:
    joblib.dump(best_model_for_prediction, model_filename)
    print(f"Model berhasil disimpan sebagai '{model_filename}'")

    # CEK UKURAN FILE MODEL
    file_size_model = os.path.getsize(model_filename)
    print(f"Ukuran file model: {file_size_model} bytes")
    if file_size_model == 0:
        print("PERINGATAN: File model 0 bytes! Ada masalah saat menyimpan!")
    elif file_size_model < 1000:
         print("INFO: File model Linear Regression biasanya kecil, ukuran ini wajar.")
except Exception as e:
    print(f"ERROR FATAL SAAT MENYIMPAN MODEL: {e}")

# Unduh requirements.txt
!pip freeze > requirements.txt

"""## Data Preparation / Preprocessing (Berdasarkan Test.csv)

**Fungsi cell ini:**

- Mengubah tipe data
- Penanganan missing values (data yang hilang).
- Penanganan data duplikat.
- Ubah nilai data dari kolom `sunshine_duration` terkait validasi satuan.
- Melakukan feature engineering.
- Melakukan encoding data kategorikal.
- Melakukan standarisasi

**MENGUBAH TIPE DATA**
"""

df_test['wind_direction_10m_dominant'] = df_test['wind_direction_10m_dominant'].round().astype('Int64')
df_test['date'] = pd.to_datetime(df_test['date'])

# Cek Jumlah Dataframe
df_test.shape

"""**PENANGANAN MISSING VALUE**

"""

print("\nJumlah Nilai Hilang di Data Test:")
print(df_test.isnull().sum())

"""Tidak ada missing value ketika data understanding sehingga tidak dilakukan penanganan.

**PENANGANAN DATA DUPLIKAT**
"""

df_test.duplicated().sum()

df_test1 = df_test.drop('ID', axis=1)
df_test1.head()

"""Tidak ada data duplikat sehingga tidak dilakukan penanganan.

**UBAH SATUAN NILAI KOLOM `sunshine_duration`**
"""

# --- Konversi sunshine_duration dari Detik ke Jam ---
# 1 jam = 3600 detik

df_test1['sunshine_duration'] = df_test1['sunshine_duration'] / 3600

print("\n--- Setelah Konversi sunshine_duration ke jam ---")
print(df_test1['sunshine_duration'])
print("\nStatistik deskriptif sunshine_duration setelah konversi:")
print(df_test1['sunshine_duration'].describe())

"""**MELAKUKAN FEATURE ENGINEERING**

Akan diubah kolom `date` menjadi beberapa kolom seperti `month`, `year`,`day`.
"""

df_test1['year'] = df_test1['date'].dt.year
df_test1['month'] = df_test1['date'].dt.month_name()
df_test1['day_of_week'] = df_test1['date'].dt.day_name()

print("\n--- DataFrame Setelah Penambahan Kolom Baru ---")
print(df_test1.head())

print("\n--- Cek Unik Nilai pada Kolom Baru (Tipe Kategorikal) ---")
print(f"Unik 'year': {df_test1['year'].unique()}")
print(f"Unik 'month': {df_test1['month'].unique()}")
print(f"Unik 'day_of_week': {df_test1['day_of_week'].unique()}")

"""**EXLORATORY DATA ANALYSIS**"""

# ANALISIS KOLOM NUMERIK (BOXPLOT)

# Dapatkan semua kolom numerik
numerical_cols = df_test1.select_dtypes(include=np.number).columns.tolist()

# Buat box plot untuk setiap kolom numerik
plt.figure(figsize=(20, 15))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(5, 6, i)
    sns.boxplot(y=df_test1[col])
    plt.title(col, fontsize=10)
    plt.tight_layout()

plt.show()

# PENANGANAN OUTLIER

print("--- Statistik Deskriptif SEBELUM Capping IQR ---")
cols_to_cap = [
    'wind_speed_10m_max',
    'wind_gusts_10m_max',
    'et0_fao_evapotranspiration'
]
print(df_test1[cols_to_cap].describe())

# --- 1. Penanganan Outlier menggunakan Capping IQR (hanya batas atas) ---

for col in cols_to_cap:
    Q1 = df_test1[col].quantile(0.25)
    Q3 = df_test1[col].quantile(0.75)
    IQR = Q3 - Q1
    upper_bound = Q3 + 1.5 * IQR

    df_test1[col] = df_test1[col].clip(upper=upper_bound, lower=df_train1[col].min())
    print(f"Menerapkan capping IQR ke '{col}'. Batas Atas: {upper_bound:.2f}")

# --- 2. Verifikasi Setelah Penanganan Outlier (Capping IQR) ---
print("\n--- Statistik Deskriptif SETELAH Capping IQR ---")
print(df_test1[cols_to_cap].describe())

# --- Visualisasi ---
plt.figure(figsize=(15, 10))
for i, col in enumerate(cols_to_cap):
    plt.subplot(2, 3, i + 1)
    sns.boxplot(y=df_test1[col])
    plt.title(f'Capped: {col}')

plt.tight_layout()
plt.show()

print("\nDataFrame dengan kolom yang sudah dicapping (beberapa baris awal):")
print(df_test1[cols_to_cap].head())

"""**ENCODING KOLOM KATEGORIKAL**

Akan dilakukan encoding pada kolom kategorikal yaitu kolom `cluster_id`, `year`, `month`, `day_of_week` menggunakan jenis One-Hot-Encoding.
"""

categorical_columns_to_encode = ['cluster_id', 'month', 'day_of_week']

df_test1_encoded = pd.get_dummies(df_test1,
                            columns=categorical_columns_to_encode)

newly_created_dummy_cols = [col for col in df_test1_encoded.columns
                            if any(col.startswith(cat_col + '_') for cat_col in categorical_columns_to_encode)]

for col in newly_created_dummy_cols:
    if df_test1_encoded[col].dtype == bool:
        df_test1_encoded[col] = df_test1_encoded[col].astype(int)


print("\n--- DataFrame df_test1_encoded setelah One-Hot Encoding ---")
print(df_test1_encoded)
print("\nTipe data setelah encoding:")
print(df_test1_encoded.dtypes)

"""**MELAKUKAN STANDARISASI** CEK LAGIIII NANTIIIIIIIIIIIIIII

Tujuan dilakukan standarisasi adalah menyamakan skala fitur dan meningkatkan kinerja model tertentu.

**UNDUH DATA HASIL PREPARATION DARI DATA TEST**

**MELAKUKAN PREDIKSI**

Prediksi akan dilakukan melalui deployment streamlit dengan URL berikut:

## Memasukkan Hasil Prediksi (Pada Submission.csv)
"""